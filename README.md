# K-Nearest-Neighbors-KNN-Classification
# Playing with Kâ€‘NN on the tiny digits set ğŸ–‹ï¸

I wanted something superâ€‘simple to refresh my memory on kâ€‘Nearestâ€‘Neighbours, so I grabbed the builtâ€‘in **digits** dataset that ships with Scikitâ€‘learn (those 8Ã—8 pixel images of handwritten numbers) and messed around with it. This repo is basically my scratchpad.

---

## Whatâ€™s inside

| File / stuff | What it is |
|--------------|------------|
| **MNIST_Digits.xlsx** | Whole dataset in Excel form (64 pixel columns + a `label`) so I can peek at it in both Pandas *and* Excel. |
| **knn_digits.ipynb**  | My Jupyter notebook â€“ load data, scale, split, train, test, plot. |
| **README.md**         | Youâ€™re reading it. |

---

## Quick notes on the data

* **Samples:** 1â€¯,â€¯797 pictures  
* **Classes:** digits **0â€‘9** (fairly balanced)  
* **Features:** 64 grayscale values (flattened 8Ã—8 image)  

Thatâ€™s small enough that Kâ€‘NN runs in a blink, but still big enough to see real patterns.

---

## How I ran things (tiny cheatâ€‘sheet)

```bash
# 1. set up environment (once)
python -m venv venv
source venv/bin/activate  # Win: venv\Scripts\activate
pip install pandas scikit-learn matplotlib jupyter

# 2. fire up the notebook
jupyter notebook knn_digits.ipynb
Inside the notebook I:

Loaded the Excel file into a DataFrame.

Scaled everything to 0â€‘1 with MinMaxScaler() (helps Kâ€‘NN a lot).

Split 75â€¯% train / 25â€¯% test.

Looped over kâ€¯=â€¯1,â€¯3,â€¯5,â€¯7,â€¯9 and printed accuracies.

Drew a confusion matrix for the best k (usually 3).

Used PCA down to 2â€‘D just to sketch the decision regions (purely for eyeâ€‘candy).

Typical test accuracy: â‰ˆâ€¯98â€¯% with k=3.

